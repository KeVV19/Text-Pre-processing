{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae67f63",
   "metadata": {},
   "source": [
    "### Kevin Setiawan Miharjo TP059076"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56d90a",
   "metadata": {},
   "source": [
    "## Q1. Form tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fe9245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "# Read Data_1.txt and close file\n",
    "txt1 = open(\"Data_1.txt\", \"r\")\n",
    "data = txt1.read()\n",
    "txt1.close()\n",
    "\n",
    "# Display text corpus\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5a81f",
   "metadata": {},
   "source": [
    "### 1.1 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk library\n",
    "!pip install nltk\n",
    "\n",
    "# Import nltk library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d00d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nltk\n",
    "#!pip install nltk\n",
    "\n",
    "# Import nltk and sent_tokenize() function\n",
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "31bb2041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.', 'However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics.', 'This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered.', 'So what should a brand do to capture that low hanging fruit?']\n"
     ]
    }
   ],
   "source": [
    "# Use sent_tokenize() function to do sentence segmentation\n",
    "tokens = nltk.sent_tokenize(data)\n",
    "\n",
    "# Display result\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760777c",
   "metadata": {},
   "source": [
    "### 1.2 Word Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0184650",
   "metadata": {},
   "source": [
    "#### Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0228040c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '\"contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information\"', 'in', 'source', 'material,', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand,', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations.', 'However,', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit?']\n"
     ]
    }
   ],
   "source": [
    "# Call split() function and assign to variable\n",
    "split_tokens = data.split()\n",
    "\n",
    "# Display result\n",
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37cdfa8",
   "metadata": {},
   "source": [
    "#### Regular Expression (re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe7ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import re package\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d57e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', 'in', 'source', 'material', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', 'However', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "# Use findall() function to match patterns in a string\n",
    "# [\\w]+ is passed to match all word characters (letters and digits) \n",
    "re_tokens = re.findall(\"[\\w]+\", data)\n",
    "\n",
    "# Display result\n",
    "print(re_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19272955",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4114f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import word_tokenize() function from nltk library\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5252b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "# Use word_tokenize() function and assign to variable\n",
    "nltk_tokens = nltk.word_tokenize(data)\n",
    "\n",
    "# Display result\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff615f",
   "metadata": {},
   "source": [
    "#### Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f9fdc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<-SPLIT FUNCTION->\n",
      "\"contextual\n",
      "information\"\n",
      "material,\n",
      "brand,\n",
      "conversations.\n",
      "However,\n",
      "metrics.\n",
      "discovered.\n",
      "fruit?\n",
      "\n",
      " <-REGULAR EXPRESSION->\n",
      "contextual\n",
      "information\n",
      "material\n",
      "conversations\n",
      "However\n",
      "metrics\n",
      "discovered\n",
      "fruit\n",
      "\n",
      " <-NLTK->\n",
      "``\n",
      "contextual\n",
      "information\n",
      "''\n",
      "material\n",
      ",\n",
      "conversations\n",
      ".\n",
      "However\n",
      "metrics\n",
      "discovered\n",
      "fruit\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "# Declare 3 lists to compare the output difference\n",
    "diff1= []\n",
    "diff2= []\n",
    "diff3= []\n",
    "\n",
    "# Below each techniques is compared using simple for loop\n",
    "# Difference found in split function\n",
    "for i in split_tokens:\n",
    "    if i not in re_tokens:\n",
    "        if i not in diff1:\n",
    "            diff1.append(i)\n",
    "    elif i not in nltk_tokens:\n",
    "        if i not in diff1:\n",
    "            diff1.append(i)\n",
    "\n",
    "# Difference found in Regular Expression\n",
    "for j in re_tokens:\n",
    "    if j not in split_tokens:\n",
    "        if j not in diff2:\n",
    "            diff2.append(j)\n",
    "    elif j not in nltk_tokens:\n",
    "        if j not in diff2:\n",
    "            diff2.append(j)\n",
    "\n",
    "# Difference found in nltk Packages\n",
    "for k in nltk_tokens:\n",
    "    if k not in split_tokens:\n",
    "        if k not in diff3:\n",
    "            diff3.append(k)\n",
    "    elif k not in re_tokens:\n",
    "        if k not in diff3:\n",
    "            diff3.append(k)\n",
    "\n",
    "# Display the result from each list\n",
    "# By doing this, clear differences can be spotted more easily from the outputs\n",
    "print('<-SPLIT FUNCTION->')\n",
    "for x in diff1:\n",
    "    print(x)\n",
    "print('\\n','<-REGULAR EXPRESSION->')\n",
    "for y in diff2:\n",
    "    print(y)\n",
    "print('\\n','<-NLTK->')\n",
    "for z in diff3:\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397ad96",
   "metadata": {},
   "source": [
    "##  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75879667",
   "metadata": {},
   "source": [
    "## Q2. Form word stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62672ba1",
   "metadata": {},
   "source": [
    "#### Regular Expression Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42ab84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['senti', 'analysi', 'is', 'context', 'min', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'information', 'in', 'sourc', 'material', 'and', 'help', 'a', 'busines', 'to', 'understand', 'the', 'social', 'senti', 'of', 'their', 'brand', 'product', 'or', 'servic', 'whil', 'monitor', 'onlin', 'conversation', 'however', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'senti', 'analysi', 'and', 'count', 'bas', 'metric', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'thos', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discover', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "# Import re package for word tokenization\n",
    "# Import RegexpStemmer() module from nltk.stem module\n",
    "import re\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Assign parameters to stemmer function\n",
    "# The paratemers include the list of affixes that want to be stemmed \n",
    "# and the minimum word length to be stemmed\n",
    "regexp = RegexpStemmer('ing$|s$|e$|es$|able$|ed$|ual$|ly$|ment$|ive$', min=4)\n",
    "\n",
    "# Before stemming, word tokenization is needed to stem each words\n",
    "words = re.findall(\"[\\w]+\", data)\n",
    "\n",
    "# Display result\n",
    "print([regexp.stem(w.lower()) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066f583",
   "metadata": {},
   "source": [
    "#### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5226b7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysi', 'is', 'contextu', 'mine', 'of', 'text', 'which', 'identifi', 'and', 'extract', 'subject', 'inform', 'in', 'sourc', 'materi', 'and', 'help', 'a', 'busi', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'servic', 'while', 'monitor', 'onlin', 'convers', 'howev', 'analysi', 'of', 'social', 'media', 'stream', 'is', 'usual', 'restrict', 'to', 'just', 'basic', 'sentiment', 'analysi', 'and', 'count', 'base', 'metric', 'thi', 'is', 'akin', 'to', 'just', 'scratch', 'the', 'surfac', 'and', 'miss', 'out', 'on', 'those', 'high', 'valu', 'insight', 'that', 'are', 'wait', 'to', 'be', 'discov', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'captur', 'that', 'low', 'hang', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "# Import re package for word tokenization\n",
    "# Import PorterStemmer() module from nltk.stem module\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Assign stemmer function to a variable\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Before stemming, word tokenization is needed to stem each words\n",
    "words = re.findall(\"[\\w]+\", data)\n",
    "\n",
    "# Display result\n",
    "# Call stemmer function for every tokens\n",
    "print([ps.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219989e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec42f0",
   "metadata": {},
   "source": [
    "## Q3. Filter stop words and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78488141",
   "metadata": {},
   "source": [
    "#### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747d2fb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ourselves', 'had', 'should', 'during', 'your', 'was', 'don', 've', \"couldn't\", 'being', \"it's\", 'a', 'down', 'hadn', 'as', 'having', 'up', 'will', 'hasn', 'are', 'through', 'more', 'yourselves', 'has', 'in', 'am', 'to', \"shouldn't\", 'then', 'yours', 'doing', 'my', 'she', 'about', 't', 'now', \"wouldn't\", 'can', 'who', 'again', 'between', 'most', 'for', \"won't\", 'll', 'we', 'it', 'shouldn', 'y', \"haven't\", 'themselves', 'so', 'the', 'both', 'o', 'isn', 'by', 's', 'on', 'their', 'too', 'theirs', \"weren't\", 'any', 'if', 'i', \"you'll\", \"hasn't\", 'is', \"you're\", 'mustn', 'do', 'after', 'shan', 'until', 'have', 'myself', 'which', 'd', 'didn', 'couldn', 'how', 'than', 'against', 'me', 'or', 'above', 'all', \"should've\", 'itself', 'those', \"didn't\", 'before', 'own', 'what', 'hers', 'because', 'ours', 'our', 'herself', 'with', 'haven', 'her', 'while', 'under', 'each', 'yourself', 'you', 'm', \"that'll\", 'himself', \"needn't\", 'him', 'his', 'aren', 'further', 'into', \"doesn't\", 'from', 'just', \"you've\", 'below', 'nor', 'here', 'does', \"hadn't\", 'these', 'few', 'such', 'where', 'once', \"she's\", 'ma', 'very', 'needn', 'off', 'won', \"isn't\", \"shan't\", \"wasn't\", 'an', 'doesn', \"mightn't\", 'and', 'were', 'of', 'them', 're', 'wouldn', 'not', 'only', 'at', \"don't\", 'they', \"aren't\", 'mightn', 'weren', 'ain', 'same', 'he', 'no', 'this', 'be', 'over', 'there', \"you'd\", 'why', 'whom', 'when', 'some', 'wasn', 'other', 'out', 'been', 'but', 'that', \"mustn't\", 'its', 'did'}\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords() module from nltk library\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Import stopwords() module from nltk.corpus package\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# List all english stopwords in a set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Display result\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fb1326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis \"contextual mining text identifies extracts subjective information\" source material, helping business understand social sentiment brand, product service monitoring online conversations. however, analysis social media streams usually restricted basic sentiment analysis count based metrics. akin scratching surface missing high value insights waiting discovered. brand capture low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "# Convert string to list of words\n",
    "lst_string = data.lower().split()\n",
    "\n",
    "# Declare a list to store stop words found in text corpus\n",
    "stop_words_found = []\n",
    "\n",
    "# Declare an empty string to display text corpus without stop words\n",
    "no_stopwords_string = \"\"\n",
    "\n",
    "for i in lst_string:\n",
    "    # Append stop words found in text corpus to list\n",
    "    if i in stop_words:\n",
    "        if i not in stop_words_found:\n",
    "            stop_words_found.append(i)\n",
    "    # Add non-stopwords into declared empty string variable\n",
    "    if not i in stop_words:\n",
    "        no_stopwords_string += i+ ' '\n",
    "        \n",
    "# Remove last space\n",
    "no_stopwords_string = no_stopwords_string[:-1]\n",
    "\n",
    "# Display text corpus without stop words\n",
    "print(no_stopwords_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5eeeae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'of', 'which', 'and', 'in', 'a', 'to', 'the', 'their', 'or', 'while', 'just', 'this', 'out', 'on', 'those', 'that', 'are', 'be', 'so', 'what', 'should', 'do']\n"
     ]
    }
   ],
   "source": [
    "# Display list of stop words founded in text corpus\n",
    "print(stop_words_found)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b03fb2",
   "metadata": {},
   "source": [
    "#### Punctuations removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f26b9200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis contextual mining text identifies extracts subjective information source material helping business understand social sentiment brand product service monitoring online conversations however analysis social media streams usually restricted basic sentiment analysis count based metrics akin scratching surface missing high value insights waiting discovered brand capture low hanging fruit\n"
     ]
    }
   ],
   "source": [
    "# Import regular expression package\n",
    "import re\n",
    "\n",
    "# Use sub() function to replace punctuations with empty string\n",
    "# [^\\w\\s] is passed to match all non-word ana non-whitespace characters (punctuations)\n",
    "no_punc_string = re.sub('[^\\w\\s]','',no_stopwords_string)\n",
    "\n",
    "# Display result\n",
    "print(no_punc_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9b24e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe039f3",
   "metadata": {},
   "source": [
    "## Q4. Form Parts of Speech (POS) taggers & Syntactic Analysers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a78b7034",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A videogame or computergame is an electronic-game that involves interaction with a user interface or input device\n"
     ]
    }
   ],
   "source": [
    "# Read Data_2.txt and close file\n",
    "txt2 = open(\"Data_2.txt\", \"r\")\n",
    "data2 = txt2.read()\n",
    "txt2.close()\n",
    "\n",
    "# Display data\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe44e6",
   "metadata": {},
   "source": [
    "#### NLTK POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07de6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk library\n",
    "import nltk\n",
    "\n",
    "# Download punkt and averaged_perceptron_tagger from nltk library\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Import pos_tag() function from nltk library\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "600e07ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic-game', 'JJ'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Do word tokenization to enable each word POS tagging\n",
    "tokens = word_tokenize(data2)\n",
    "\n",
    "# POS Tag each word and display result\n",
    "pos_tokens = pos_tag(tokens) \n",
    "print(pos_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a0b86",
   "metadata": {},
   "source": [
    "#### Regular Expression Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0e244c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk library and re package\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Import word_tokenize, pos_tag, RegexpTagger function from nltk library\n",
    "from nltk import word_tokenize, pos_tag, RegexpTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "acd0d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'NNS'), ('an', 'DT'), ('electronic-game', 'NN'), ('that', 'DT'), ('involves', 'NNS'), ('interaction', 'NN'), ('with', 'NN'), ('a', 'DT'), ('user', 'NN'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Do word tokenization to enable each word POS tagging\n",
    "tokens = word_tokenize(data2) \n",
    "\n",
    "# RegexpTagger code referenced from: \n",
    "# https://tedboy.github.io/nlps/generated/generated/nltk.RegexpTagger.html\n",
    "\n",
    "# Call RegexpTagger function and define POS tagging criteria\n",
    "regexp_tagger = RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),           # cardinal numbers\n",
    "     (r'(The|the|A|a|An|an|this|that)$', 'DT'), # determiner\n",
    "     (r'(for|and|or|nor|but|yet|so)$', 'CC'),   # conjuctions\n",
    "     (r'.*able$', 'JJ'),                        # adjectives\n",
    "     (r'.*ness$', 'NN'),                        # nouns formed from adjectives\n",
    "     (r'.*ly$', 'RB'),                          # adverbs\n",
    "     (r'.*s$', 'NNS'),                          # plural nouns\n",
    "     (r'.*ing$', 'VBG'),                        # gerunds\n",
    "     (r'.*ed$', 'VBD'),                         # past tense verbs\n",
    "     (r'.*', 'NN')                              # nouns (default)\n",
    "    ])\n",
    "\n",
    "# Do pos tagging on each word tokens\n",
    "# Display result\n",
    "print(regexp_tagger.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed361e",
   "metadata": {},
   "source": [
    "#### Parse Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8663974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk library, download punkt & averaged_perceptron_tagger module\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Import pos_tag(), word_tokenize(), RegexpParser() function from nltk library\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b91b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each word and do pos tagging\n",
    "data2_tagged = pos_tag(word_tokenize(\"the giant elephant saw the fine fat bear in the jungle\"))\n",
    "\n",
    "# Define english grammar rules in RegexpParser() function and assign to variable\n",
    "chunker = RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>}            #To extract Prepositions\n",
    "V: {<V.*>}           #To extract Verbs\n",
    "PP: {<p> <NP>}       #To extract Prepositional Phrases\n",
    "VP: {<V> <NP|PP>*}   #To extract Verb Phrases\n",
    "                    \"\"\")\n",
    "# Use the variable and call parse on the tagged data\n",
    "# Generate the parse tree\n",
    "parse_tree = chunker.parse(data2_tagged)\n",
    "parse_tree.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
